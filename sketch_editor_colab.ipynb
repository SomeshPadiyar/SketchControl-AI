# ==============================================================================
# ðŸŽ¨ SketchControl-AI: Sketch-Guided Image Generation & Editing
#
# This notebook runs a minimal, all-in-one web application (powered by Gradio)
# that combines Stable Diffusion ControlNet (Scribble) for initial generation
# and Stable Diffusion Inpainting for editing, all on a free Google Colab GPU.
#
# Click "Runtime" -> "Change runtime type" -> Select "GPU" before starting.
# ==============================================================================

#--DOWNLOAD THESE DEPENDENCIES--
#  !pip install diffusers
#  !pip install controlnet_aux


import os
import torch
import numpy as np
from PIL import Image, ImageDraw
import gradio as gr

# diffusers pipelines & models
from diffusers import (
    StableDiffusionInpaintPipeline,
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    UniPCMultistepScheduler,
)
from controlnet_aux import HEDdetector

# ---------------- helper to show messages in UI ----------------
def make_text_image(msg: str, size=(512, 512), bg=(255, 220, 180), fg=(0, 0, 0)):
    img = Image.new("RGB", size, color=bg)
    draw = ImageDraw.Draw(img)
    y = 8
    for line in msg.splitlines():
        draw.text((8, y), line, fill=fg)
        y += 12
    return img

# ---------------- environment & device ----------------
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ---------------- install (run once in a fresh session) ----------------
print("Installing libs (if needed)...")
!pip install -q diffusers accelerate controlnet_aux transformers gradio Pillow
!pip install -q xformers || true

os.environ["MPLBACKEND"] = "Agg"

# ---------------- load models ----------------
inpaint_pipe = None
controlnet_pipe = None
controlnet_scribble = None
scribble_processor = None

# Load inpaint pipeline (for editing)
try:
    inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
        "runwayml/stable-diffusion-inpainting",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        safety_checker=None,
    )
    # scheduler swap for faster sampling (optional)
    inpaint_pipe.scheduler = UniPCMultistepScheduler.from_config(inpaint_pipe.scheduler.config)
    if device == "cuda":
        inpaint_pipe = inpaint_pipe.to(device)
    try:
        inpaint_pipe.enable_xformers_memory_efficient_attention()
    except Exception as e:
        print("xformers (inpaint) not enabled:", e)
    print("Loaded inpaint pipeline.")
except Exception as e:
    print("Error loading inpaint pipeline:", e)
    inpaint_pipe = None

# Load ControlNet model + ControlNet pipeline (for sketch->image)
try:
    controlnet_scribble = ControlNetModel.from_pretrained(
        "lllyasviel/control_v11p_sd15_scribble",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    )
    # Use SD v1.5 (or another text->image base) with ControlNet attached
    controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        controlnet=controlnet_scribble,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        safety_checker=None,
    )
    controlnet_pipe.scheduler = UniPCMultistepScheduler.from_config(controlnet_pipe.scheduler.config)
    if device == "cuda":
        controlnet_pipe = controlnet_pipe.to(device)
    try:
        controlnet_pipe.enable_xformers_memory_efficient_attention()
    except Exception as e:
        print("xformers (controlnet) not enabled:", e)
    print("Loaded ControlNet pipeline.")
except Exception as e:
    print("Error loading ControlNet pipeline:", e)
    controlnet_pipe = None
    controlnet_scribble = None

# Load HEDdetector (scribble processor)
try:
    scribble_processor = HEDdetector.from_pretrained("lllyasviel/Annotators")
    print("Loaded HEDdetector.")
except Exception as e:
    print("Error loading HEDdetector:", e)
    scribble_processor = None

# ---------------- robust input parser for ImageEditor nested data ----------------
def extract_images(data):
    """Recursively extract image (RGB PIL) and mask (L PIL) from Gradio ImageEditor output."""
    img = None
    mask = None

    if data is None:
        return None, None

    # direct PIL
    if isinstance(data, Image.Image):
        return data.convert("RGB"), None

    # numpy array
    if isinstance(data, np.ndarray):
        return Image.fromarray(data).convert("RGB"), None

    # dict (nested structure)
    if isinstance(data, dict):

        if "image" in data and isinstance(data["image"], Image.Image):
            img = data["image"].convert("RGB")
        if "mask" in data and isinstance(data["mask"], Image.Image):
            try:
                mask = data["mask"].convert("L")
            except Exception:
                mask = None
        # look into layers if present
        if img is None and "layers" in data:
            for layer in data["layers"]:
                i, m = extract_images(layer)
                if i is not None and img is None:
                    img = i
                if m is not None and mask is None:
                    mask = m

    # list/tuple
    if isinstance(data, (list, tuple)):
        for elem in data:
            i, m = extract_images(elem)
            if i is not None and img is None:
                img = i
            if m is not None and mask is None:
                mask = m

    return img, mask

# ---------------- generation function ----------------
@torch.no_grad()
def generate_image(gr_input, prompt: str, steps: int, guidance_scale: float, inpaint_strength: float, sketch_mode: bool):
    # model checks
    if controlnet_pipe is None or inpaint_pipe is None or scribble_processor is None:
        return make_text_image("Model(s) not loaded properly. See notebook logs.")

    # parse input
    input_image, mask_image = extract_images(gr_input)
    if input_image is None:
        return make_text_image("No input image found. Please draw or upload in the left panel and try again.")
    input_image = input_image.convert("RGB")

    if sketch_mode or mask_image is None or mask_image.getbbox() is None:
        # SKETCH -> ControlNet pipeline
        try:
            # create scribble control image from user sketch (HED)
            scribble_np = scribble_processor(input_image, scribble=True, detect_resolution=512, image_resolution=512)
            out = controlnet_pipe(
                prompt=prompt,
                image=scribble_np,  
                num_inference_steps=int(steps),
                guidance_scale=float(guidance_scale),
            ).images[0]
            return out
        except Exception as e:
            import traceback
            tb = traceback.format_exc()
            return make_text_image(f"ControlNet generation error:\n{type(e).__name__}: {str(e)}\n\nTraceback in notebook log.")
    else:
        # INPAINTING
        try:
            out = inpaint_pipe(
                prompt=prompt,
                image=input_image,
                mask_image=mask_image,
                strength=float(inpaint_strength),
                num_inference_steps=int(steps),
                guidance_scale=float(guidance_scale),
            ).images[0]
            return out
        except Exception as e:
            import traceback
            tb = traceback.format_exc()
            return make_text_image(f"Inpainting error:\n{type(e).__name__}: {str(e)}\n\nTraceback in notebook log.")

# ---------------- Gradio UI ----------------
with gr.Blocks(css="#output_image {max-width: 512px;}") as demo:
    gr.Markdown("## AI Sketch-Guided Image Generator\n\n- Sketch mode uses ControlNet + HED\n- Inpainting mode uses Stable Diffusion Inpainting\n")
    with gr.Row():
        with gr.Column():
            input_editor = gr.ImageEditor(label="Draw or Upload", type="pil", image_mode="RGB", interactive=True, width=512, height=512)
            prompt_box = gr.Textbox(label="Prompt", value="A photorealistic red apple on a white background, studio lighting")
            sketch_toggle = gr.Checkbox(label="Sketch Mode (ControlNet)", value=True)
        with gr.Column():
            out_im = gr.Image(label="Generated Image", width=512, height=512)
            steps_slider = gr.Slider(label="Steps", minimum=10, maximum=50, value=30, step=1)
            cfg_slider = gr.Slider(label="Guidance Scale", minimum=3.0, maximum=15.0, value=7.5, step=0.5)
            strength_slider = gr.Slider(label="Inpaint Strength (editing only)", minimum=0.5, maximum=1.0, value=0.9, step=0.05)
            run_btn = gr.Button("Generate")

    run_btn.click(fn=generate_image, inputs=[input_editor, prompt_box, steps_slider, cfg_slider, strength_slider, sketch_toggle], outputs=out_im)

print("Launching app...")
demo.launch(share=True)
